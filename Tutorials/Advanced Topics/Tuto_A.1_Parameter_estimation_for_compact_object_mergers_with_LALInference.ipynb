{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d3b778-8402-4dbf-afa8-430d0f60bd70",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img style=\"float: left;padding: 1.3em\" src=\"https://raw.githubusercontent.com/gw-odw/odw-2022/main/Tutorials/logo.png\">  \n",
    "\n",
    "#  Gravitational Wave Open Data Workshop #5\n",
    "\n",
    "#### Tutorial A.1: Parameter estimation on GW190828_063405 using GWOSC data and LALInference software.\n",
    "\n",
    "This tutorial provides step-by-step instructions to estimate the properties of the source of the gravitational events (GW) detected by the LIGO-Virgo-KAGRA (LVK) collaboration. The parameter estimation is performed with `LALInference`, a software library developed by the LVK collaboration to perform Bayesian inference of GW source properties for binary systems of black holes and/or neutron stars, and used during the three first observation runs. This tutorial can be adapted to analyse any event in the GWTC-2 & 3 catalogs. \n",
    "\n",
    "#### Content\n",
    "The tutorial provides instruction to: \n",
    "- install `LALInference`\n",
    "- download the GW strain \n",
    "- downlaod the PSD and calibration envelops\n",
    "- read the configuration options\n",
    "- start a `LALInference` run\n",
    "\n",
    "#### Note\n",
    "This tutorial is intended to be ran with a local `conda` installation on a mac or Linux computer, and does not support mybinder nor Google Colab.\n",
    "\n",
    "It is adviced to use the official `IGWN` environment, see the [documentation](https://computing.docs.ligo.org/conda/) for details and instructions.\n",
    "\n",
    "Instructions about setting up a `conda` environement are available on [this page](https://github.com/gw-odw/odw-2022/blob/main/setup.md) (option 3). \n",
    "The following packages must be installed in the conda environment: \n",
    "- `conda install --channel conda-forge lalsuite` \n",
    "- `conda install -c conda-forge pesummary`\n",
    "\n",
    "   \n",
    "#### Documentation\n",
    "- The formalism of the LALInference package is described in the following article: \n",
    "[Phys. Rev. D 91, 042003 (2015)](https://arxiv.org/abs/1409.7215)\n",
    "- A lecture of GW parameter estimation with Bayesian inference: [Recording of GWOSC ODW #2](https://www.youtube.com/watch?v=_NaAKeVJe1s&t=1s)\n",
    "- All the GWOSC open data workshops lectures: [Link to GWOSC ODW](https://www.gw-openscience.org/workshops/)\n",
    "\n",
    "#### Acknowledgements\n",
    "This tutorial is created by Leïla Haegel (leila.haegel@ligo.org), based on utilitary software designed by the LVK collaboration members. A special thanks is due to Charlie Hoy and Marc Arène for their assistance in understanding `PESummary` and `LALInference`.\n",
    "\n",
    "**Last update**: May 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aecf277-402b-4b3a-9aa2-a34b67fadc57",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  A bit of theory\n",
    "\n",
    "**Introduction**: `LALInference` [1] uses Markov chains to sample the posterior probability of the GW source parameters (for GW emitted by binary systems of black holes and/or neutron stars). \n",
    "\n",
    "**Markov chains**: Markov chains are semi-random walks in specific space, some Markov chain algorithms such as Metropolis-Hastings or Nested Sampling aim at having the semi-random walk steps be proportonal to a target distribution. In LALInference the target distribution is the joint posterior probability of the GW source parameters (such as mass, spin, binary localisation and orientation. \n",
    "\n",
    "**Posterior probability**: The posterior probability is given by the Bayes theorem: \n",
    "\n",
    "$$ p(\\theta | d, H) = \\frac{p(d|\\theta,H) \\ p(\\theta|H)}{p(d|H)} $$\n",
    "\n",
    "where $d$ are the data, $\\theta$ are the GW parameters, and $H$ the hypotheses according to which the posterior probability is computed. $p(\\theta | d, H)$ is the posterior probability, $p(\\theta|H)$ is the prior probability, $p(d|H)$ is the evidence, and $p(d|\\theta,H)$ is the likelihood given by [1]:\n",
    "\n",
    "$$ p(d|\\vec{\\theta},H_S,S_n(f),\\vec{\\eta}) = exp \\sum_i \\left[ -\\frac{2 |\\tilde{h_i}(\\vec{\\theta}) - \\tilde{d_i}|^2}{T \\eta(f_i) S_n(f_i)} - \\frac{1}{2} log(\\pi \\eta_i T S_n(f_i)/2 \\right] $$\n",
    "\n",
    "where $p(d|\\vec{\\theta},H_S,S_n(f),\\vec{\\eta})$ is the likelihood marginalised over the calibration envelops, $d_i$ is the strain and $S_n(f_i)$ is the Power Spectral Density (PSD). We therefore need all those quantities, that are described in details below. \n",
    "\n",
    "**LALInference**: `LALInference` is a software library computing the posterior probability $p(\\theta | d, H)$ with a Markov chain process. It returns the steps of the chains, i.e. the value of the posterior probability at different points of the parameter space. Their distributions enable to infer the most probable value and credible intervals of the GW source parameters. This tutorial downloads the necessary inputs and configures a `LALInference` run.\n",
    "\n",
    "\n",
    "[1] [Phys. Rev. D 91, 042003 (2015)](https://arxiv.org/abs/1409.7215)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33012497-9c29-4a93-9217-f47d05b6afa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4b9924-664f-49dd-a2b4-57e2f7cc4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Those packages will be needed for the execution\n",
    "#! pip install -q lalsuite pesummary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639a964-3924-4e84-a799-7ff575e1da83",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Important:** With Google Colab, you may need to restart the runtime after running the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ce23e-6c4e-472c-8f84-cdff05d43190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization\n",
    "\n",
    "We begin by importing some commonly used functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156684ee-3f58-4d8d-b537-abd2bfb95cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import lal\n",
    "import json\n",
    "import numpy as np\n",
    "from pesummary.gw import fetch\n",
    "from pesummary.io import read\n",
    "from pesummary.gw.file.calibration import CalibrationDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352da653-ff70-4ca1-bcf8-2989c52c3efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we create the directory in which we will work for this tutorial. All the input and output files will be stored in `tuto3.3` (modify at your convenience if needed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e059e010-6c2d-4432-8475-f599f4241b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the local workdir directory name\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# create a new directory called tuto3.3 for the run\n",
    "workdir = cwd + '/tuto_A.1/'\n",
    "os.makedirs(workdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ffb7a-bf7f-4302-afaa-d21e1503562d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the data: GW190828_063405\n",
    "\n",
    "In this notebook, we analyse GW190828_063405. Information on the event can be found on the [GWOSC page](https://www.gw-openscience.org/eventapi/html/GWTC-2/GW190828_063405/). \n",
    "\n",
    "The notebook can be adapted to analyse any other event from the [GWTC-2](https://www.gw-openscience.org/GWTC-2/), [2.1](https://www.gw-openscience.org/GWTC-2.1/) & [3](https://www.gw-openscience.org/GWTC-3/) catalogs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101b5d35-fb7e-4644-b220-40fa20531a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evt = 'GW190828_063405'\n",
    "catalog = 'GWTC-2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457dd94-0299-4e86-bd9b-2b2900f83d4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - PESummary files\n",
    "\n",
    "The PE files contains the PSD, calibration envelops and inference run configuration options, as well as the posterior samples from the `LALInference` runs performed by the LVK collaboration.\n",
    "\n",
    "The file is retrieved and parsed using the `pesummary` package (more information on the [PESummary documentation](https://lscsoft.docs.ligo.org/pesummary/unstable_docs/index.html)).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ce8e2-d305-4c23-82d5-2035426ee092",
   "metadata": {
    "tags": []
   },
   "source": [
    "We start by downloading the PESummary files in the working directory (they will be saved in a directory named as the event): \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df5e9ac-df05-4f2f-80a9-8fad9d2f86d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all files from /tmp/astropy-download-505085-9k8b2xzt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch.fetch_open_samples(evt, catalog=catalog, \n",
    "                         unpack=True, read_file=False, delete_on_exit=False, \n",
    "                         outdir=workdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c252fd5-186b-4dad-8345-5cb4ad8257fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we open it, after checking that it is downloaded as expected: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51f9d7d-e18d-48ab-b30b-57b3d88df022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter estimation file found in: /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405/GW190828_063405.h5\n",
      "Opening it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leila.haegel/.conda/envs/lalsuite7.5/lib/python3.9/site-packages/pesummary/utils/dict.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.samples = np.array(_samples)\n"
     ]
    }
   ],
   "source": [
    "pe_path = workdir + evt + '/' + evt + '.h5'\n",
    "\n",
    "if os.path.exists(pe_path): \n",
    "    print('Parameter estimation file found in: ' + pe_path + '\\nOpening it.')\n",
    "    pe_data = read(pe_path, package=\"gw\")\n",
    "else: \n",
    "    print('Error: parameter estimation file not found in: ' + pe_path + \n",
    "          '\\n You need to download the PE file manually from the GWOSC: ' + \n",
    "          'https://www.gw-openscience.org/eventapi/html/'+ catalog + '/' + evt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bdffd-7b0f-4469-8d95-01bbb8d34ee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Several analyses are included in the PESummary file. \n",
    "We will chose the analysis: \n",
    "- starting with `C01` (release-level calibration)  \n",
    "- using the `IMRPhenomPv2` approximant. The approximant refers to the model used to simulate the gravitational waveform for the analysis. It does not impact the extraction of the PSD nor the calibration file, but will impact the options chosen to run the parameter estimation later in the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cebd2d-8149-461f-8265-af0f7d99e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C01:IMRPhenomPv2 found in the list of analyses. Analyses available are:  ['C01:IMRPhenomD', 'C01:IMRPhenomPv2', 'C01:NRSur7dq4', 'C01:SEOBNRv4P', 'C01:SEOBNRv4PHM', 'PrecessingSpinIMR', 'PrecessingSpinIMRHM', 'PublicationSamples', 'ZeroSpinIMR']\n"
     ]
    }
   ],
   "source": [
    "label = 'C01:IMRPhenomPv2'\n",
    "\n",
    "if label in pe_data.labels: \n",
    "    print(label + ' found in the list of analyses. Analyses available are: ', pe_data.labels)\n",
    "else: \n",
    "    print(label + ' not in the list of analyses. Analyses available are: ', pe_data.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb6ee1-b356-4842-b45b-189a4d9dda7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - PSD files\n",
    "\n",
    "The Power Spectral Density (PSD) is the distribution of the signal power in the frequency domain. The PSD can be obtained from the PE samples file using `pesummary`. The PSD must be downloaded with 1 file / interferometer as done below (any directory can be specfied, working directory used in this example). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5dc0a7-9340-45d4-9f01-736bb51e1b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSD file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_PSDs_H1.dat\n",
      "PSD file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_PSDs_L1.dat\n",
      "PSD file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_PSDs_V1.dat\n"
     ]
    }
   ],
   "source": [
    "psds = pe_data.psd[label]\n",
    "ifos = psds.detectors\n",
    "\n",
    "psd_files = {}\n",
    "\n",
    "for ifo in ifos: \n",
    "    freq = psds[ifo].frequencies\n",
    "    psd  = psds[ifo].strains\n",
    "    \n",
    "    psd_path = workdir + evt + '_PSDs_' + ifo + '.dat'\n",
    "    psd_files[ifo] = psd_path\n",
    "\n",
    "    np.savetxt(psd_path , np.column_stack([freq, psd]), delimiter='\\t')\n",
    "    \n",
    "    print('PSD file downloaded in: ', psd_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf13511-846a-425e-b163-0ca2e2cbeef1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Calibration envelops files\n",
    "\n",
    "The calibration envelops represent the uncertainty due to the detector calibration. The files can be obtained from the PE samples file using `pesummary`. They must be downloaded with 1 file / interferometer as done below (any directory can be specfied, working directory used in this example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75be22e9-83a7-419d-9974-45a87332bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_CalEnv_H1.txt\n",
      "Calibration file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_CalEnv_L1.txt\n",
      "Calibration file downloaded in:  /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/GW190828_063405_CalEnv_V1.txt\n"
     ]
    }
   ],
   "source": [
    "calib_envs = pe_data.priors['calibration'][label]\n",
    "calib_data = CalibrationDict(calib_envs)\n",
    "ifos = calib_data.detectors\n",
    "\n",
    "calib_files = {}\n",
    "\n",
    "for ifo in ifos: \n",
    "\n",
    "    freq = calib_data[ifo].frequencies\n",
    "    mag  = calib_data[ifo].magnitude\n",
    "    phi  = calib_data[ifo].phase\n",
    "    mag_m1s = calib_data[ifo].magnitude_lower\n",
    "    phi_m1s = calib_data[ifo].phase_lower\n",
    "    mag_p1s = calib_data[ifo].magnitude_upper\n",
    "    phi_p1s = calib_data[ifo].phase_upper\n",
    "    \n",
    "    calib_path = workdir + evt + '_CalEnv_' + ifo + '.txt'\n",
    "    calib_files[ifo] = calib_path\n",
    "    col_title = 'Frequency Median Mag Phase (Rad) -1 Sigma Mag -1 Sigma Phase +1 Sigma Mag +1 Sigma Phase'\n",
    "    np.savetxt(calib_path, np.column_stack([freq, mag, phi, mag_m1s, phi_m1s, mag_p1s, phi_p1s]), delimiter=' ', header=col_title)\n",
    "    \n",
    "    print('Calibration file downloaded in: ', calib_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1942b95-b260-4302-ae97-9514ba1af1d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Strain files\n",
    "\n",
    "The strain files are available on the GWOSC website and can be downloaded below using the `pesummary` package. \n",
    "The functions options refer to the following signal properties: \n",
    "\n",
    "**Duration**: Up to GWTC-3, all the gravitational waves detected from binary black hole systems have signal duration < 32 s.\n",
    "For binary systems containing one or two neutron stars, the signal is longer and strain files of 4096 s must be used. \n",
    "\n",
    "**Sampling rate**: The sampling rate can be determined by approximating that the maximal frequency $f_{max}$ to the frequency at the innermost stable circular orbit $f_{ISCO}$:  $ f_{max} \\simeq f_{ISCO}  \\simeq \\frac{4.4 kHz}{M_{tot}} $\n",
    "\n",
    "For a total mass $ M_{tot} > 4$ for binary black holes, we find the higher bound on the maximal frequency: \n",
    "$ f_{max} \\simeq 1048 Hz $\n",
    "\n",
    "The sampling rate corresponds to the Nyquist frequency: \n",
    "$f_{Nyquist} = 2 f_{max}  = 2048 Hz$\n",
    "\n",
    "so the files with a sampling rate of $ f_{Nyquist} = 4096 Hz$ can be used for any binary system of black holes detected by the LVK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eac34c-a1fc-41f8-8c32-bd93b0cde191",
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_files = {}\n",
    "\n",
    "for ifo in ifos: \n",
    "    \n",
    "    strain = fetch.fetch_open_strain(evt, IFO=ifo, duration=32, sampling_rate=4096., format='gwf', \n",
    "                                     delete_on_exit=True, read_file=False, outdir=workdir)\n",
    "    strain_files[ifo] = str(strain)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29015992-bdb2-486d-a5d5-6e632fa97173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The strain must be passed to `LALInference` using cache files, created with a bash command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f46f5246-250b-44fd-b227-6c5f15eb7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/H-H1_GWOSC_4KHZ_R1-1251009248-32.gwf | lalapps_path2cache > /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/H-H1_GWOSC_4KHZ_R1-1251009248-32.lcf\n",
      "ls /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/L-L1_GWOSC_4KHZ_R1-1251009248-32.gwf | lalapps_path2cache > /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/L-L1_GWOSC_4KHZ_R1-1251009248-32.lcf\n",
      "ls /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/V-V1_GWOSC_4KHZ_R1-1251009248-32.gwf | lalapps_path2cache > /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/V-V1_GWOSC_4KHZ_R1-1251009248-32.lcf\n"
     ]
    }
   ],
   "source": [
    "strain_cache_files = {}\n",
    "\n",
    "for ifo in ifos: \n",
    "    strain_cache_files[ifo] = strain_files[ifo].split('.gwf')[0]+ '.lcf'\n",
    "    command = 'ls ' + strain_files[ifo] +' | lalapps_path2cache > ' + strain_cache_files[ifo] \n",
    "    print(command)\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ea7e7-e8c6-43c7-b994-74c1c8dfb901",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chose the LALInference options\n",
    "\n",
    "The `LALInference` software performs parameter estimation with Bayesian inference, using Markov chain techniques. Two types of Markov chaim algorithms are implemented : Markov Chain Monte-Carlo (MCMC) and nested sampling. `LALInference` supports a wide range of options, from the input files specification to the prior to set up to the number of parallel inference runs to launch. All those options are stored in the config section of the `PESummary` files. In the notebook, we read the  `PESummary` file to create a bash file that will start the inference on our machine with the desired options.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f30f8-e5f7-4cd3-a165-615a183c7902",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will use the information in the sections belows to set up our bash file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21cdae4d-23c8-4335-9812-15115b534e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analysis', 'bayeswave', 'condor', 'data', 'datafind', 'engine', 'input', 'lalinference', 'ligo-skymap-from-samples', 'ligo-skymap-plot', 'mpi', 'paths', 'resultspage', 'skyarea', 'statevector'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_data.config[label].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4c341-fe48-4412-bc2f-f5304ae1256b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Create the bash file\n",
    "\n",
    "We know create a bash file that we will write with the necessary options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9d6d77-6522-4212-b8ed-065666b4dc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/lalinference_run.sh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_path = workdir + 'lalinference_run.sh'\n",
    "run_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e1dc09-7a5c-4ff4-8d7a-911da6e36488",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the inference executable\n",
    "\n",
    "First specify the `conda` environment where LAL is installed. The path needs to be modified according to your machine, specifying the link to the virtual environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b93121-cb6d-4b34-80cb-f7362fa4b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py38/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfced366-c9b5-421e-8ed0-ec36e8f488bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we chose the executable and relevent options from the `analysis` and `engine` sections of the config options.\n",
    "\n",
    "**For nested sampling**, we need to specify: \n",
    "- the number of live points `nlive` \n",
    "- the number of points `maxmcmc` for independent MCMC chains (optional)\n",
    "- the tolerance on the evidence `tolerance` to obtain to stop the inference (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1105f43c-e9fe-4979-8c99-2a2dd4cf2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pe_data.config[label]['analysis']['engine'] == 'lalinferencenest': \n",
    "    with open(run_path, \"w\") as run_file:\n",
    "        print(env_path + 'bin/lalinference_nest \\\\', file=run_file)\n",
    "        print('--nlive ' + pe_data.config[label]['engine']['nlive'] + ' \\\\', file=run_file)\n",
    "        if 'maxmcmc' in pe_data.config[label]['engine']: \n",
    "            print('--maxmcmc ' + pe_data.config[label]['engine']['maxmcmc'] + ' \\\\', file=run_file)\n",
    "        if 'tolerance' in pe_data.config[label]['engine']: \n",
    "            print('--tolerance ' + pe_data.config[label]['engine']['tolerance'] + ' \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45dc928-a002-43e2-a4c4-cfbd18638835",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "**For MCMC**, we need to specify: \n",
    "- the path to the MPI executable for parallelisation\n",
    "- the number of parallel jobs `np`\n",
    "- the number of independent samples `neff` to obtain to stop the inference (optional)\n",
    "- the number of temperature chains `ntemps` (optional)\n",
    "- the option to adapt the temperature chains `adapt-temps` (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e173d5e-0059-44c4-8402-16488ecaba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pe_data.config[label]['analysis']['engine'] == 'lalinferencemcmc': \n",
    "    with open(run_path, \"w\") as run_file:\n",
    "        print(env_path + 'bin/lalinference_mpi_wrapper \\\\', file=run_file)\n",
    "        print('--mpirun ' + env_path + 'bin/mpirun \\\\', file=run_file)\n",
    "        print('--executable ' + env_path + 'bin/lalinference_mcmc \\\\', file=run_file)\n",
    "        print('--np ' + pe_data.config[label]['engine']['np'] + ' \\\\', file=run_file)\n",
    "\n",
    "        if 'neff' in pe_data.config[label]['engine']: \n",
    "            print('--neff ' + pe_data.config[label]['engine']['neff'] + ' \\\\', file=run_file)\n",
    "        if 'ntemps' in pe_data.config[label]['engine']: \n",
    "            print('--ntemps ' + pe_data.config[label]['engine']['ntemps'] + ' \\\\', file=run_file)\n",
    "        if 'adapt-temps' in pe_data.config[label]['engine']: \n",
    "            print('--adapt-temps \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0027a2c-8f1c-4c22-9ce1-92f4a18aad19",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the input files\n",
    "\n",
    "Specify the location of the input files for each interferometer, with the initial frequency extracted from the `lalinference` section of the configuration options.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851b330f-4daa-47ec-9045-b92a87752bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'H1' in ifos: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--ifo H1 \\\\', file=run_file)\n",
    "        print('--H1-cache ' + strain_cache_files['H1'] + ' \\\\', file=run_file)\n",
    "        print('--H1-channel H1:GWOSC-4KHZ_R1_STRAIN \\\\', file=run_file)\n",
    "        if 'flow' in pe_data.config[label]['lalinference']: \n",
    "            f_low = pe_data.config[label]['lalinference']['flow'].replace('\\'', '\\\"')\n",
    "            print('--H1-flow ' + str(json.loads(f_low)['H1']) + ' \\\\', file=run_file)\n",
    "        print('--H1-psd ' + psd_files['H1'] + ' \\\\', file=run_file)\n",
    "        print('--H1-spcal-envelope ' + calib_files['H1'] + ' \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4451a90-c53f-436d-854f-75ce37862d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'L1' in ifos: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--ifo L1 \\\\', file=run_file)\n",
    "        print('--L1-cache ' + strain_cache_files['L1'] + ' \\\\', file=run_file)\n",
    "        print('--L1-channel L1:GWOSC-4KHZ_R1_STRAIN \\\\', file=run_file)\n",
    "        if 'flow' in pe_data.config[label]['lalinference']: \n",
    "            f_low = pe_data.config[label]['lalinference']['flow'].replace('\\'', '\\\"')\n",
    "            print('--L1-flow ' + str(json.loads(f_low)['L1']) + ' \\\\', file=run_file)\n",
    "        print('--L1-psd ' + psd_files['L1'] + ' \\\\', file=run_file)\n",
    "        print('--L1-spcal-envelope ' + calib_files['L1'] + ' \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bdc7c49-5b23-4f7a-b3ca-30943e456072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'V1' in ifos: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--ifo V1 \\\\', file=run_file)\n",
    "        print('--V1-cache ' + strain_cache_files['V1'] + ' \\\\', file=run_file)\n",
    "        print('--V1-channel V1:GWOSC-4KHZ_R1_STRAIN \\\\', file=run_file)\n",
    "        if 'flow' in pe_data.config[label]['lalinference']: \n",
    "            f_low = pe_data.config[label]['lalinference']['flow'].replace('\\'', '\\\"')\n",
    "            print('--V1-flow ' + str(json.loads(f_low)['V1']) + ' \\\\', file=run_file)\n",
    "        print('--V1-psd ' + psd_files['V1'] + ' \\\\', file=run_file)\n",
    "        print('--V1-spcal-envelope ' + calib_files['V1'] + ' \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732b707-67fb-4870-9770-a31b82897784",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the calibration \n",
    "\n",
    "Chose if the calibration envelops are sampled during the run, and specify the number of nodes from the `engine` section of the configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fb27f0-9f6f-4bd1-bd0f-fc944fc6036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'enable-spline-calibration' in pe_data.config[label]['engine']: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--enable-spline-calibration \\\\', file=run_file)\n",
    "        \n",
    "if 'spcal-nodes' in pe_data.config[label]['engine']: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--spcal-nodes ' + pe_data.config[label]['engine']['spcal-nodes'] + ' \\\\', file=run_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814003a-0a06-4484-a17e-eddf405f3b48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the signal options\n",
    "\n",
    "The options are extracted from the `engine` section of the configuration options, including: \n",
    "- the length of the data segment to analyse `seglen` (in seconds)\n",
    "- the sampling rate `srate` (in Herts)\n",
    "- the waveform approximant `approx`\n",
    "- the reference frequency `fref` (optional)\n",
    "\n",
    "The trigger time `trigtime` that must be copied from the [GWOSC page](https://www.gw-openscience.org/eventapi/html/GWTC-2/GW190828_063405/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd8a36cd-beb6-4be4-a95f-9eed74638c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "trig_time = 1251009263.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9db189-7965-4763-bd71-e40c40ce16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_path, \"a\") as run_file:\n",
    "    print('--trigtime ' + str(trig_time) + ' \\\\', file=run_file)\n",
    "    print('--srate ' + pe_data.config[label]['engine']['srate'] + ' \\\\', file=run_file)\n",
    "    print('--seglen ' + pe_data.config[label]['engine']['seglen'] + ' \\\\', file=run_file)\n",
    "    print('--approx ' + pe_data.config[label]['engine']['approx'] + ' \\\\', file=run_file)\n",
    "\n",
    "if 'fref' in pe_data.config[label]['engine']: \n",
    "    with open(run_path, \"a\") as run_file:\n",
    "        print('--fref ' + pe_data.config[label]['engine']['fref'] + ' \\\\', file=run_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52dfb35-cdad-4391-aec4-0beea942f724",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the priors\n",
    "\n",
    "The priors are specified in the `engine` section of the configuration options, with the bounds of syntax: `[param]-min` and `[param]-max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d51317-7223-4c2c-8946-ba16becf9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pe_data.config[label]['engine']:\n",
    "    if '-min'in key: \n",
    "        with open(run_path, \"a\") as run_file:\n",
    "            print('--' + key + ' ' + pe_data.config[label]['engine'][key] + ' \\\\', file=run_file)\n",
    "    if '-max'in key: \n",
    "        with open(run_path, \"a\") as run_file:\n",
    "            print('--' + key + ' ' + pe_data.config[label]['engine'][key] + ' \\\\', file=run_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bbb20-08e3-48b6-bdd3-57f7d61fb956",
   "metadata": {
    "tags": []
   },
   "source": [
    "### - Specify the output options\n",
    "\n",
    "The posterior samples will be saved in `lalinference_samples.hdf5` and we will specify options to resume the inference if interupted and to print the progress into `lalinference.out` and `lalinference.err` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5db0cab-9259-4bf1-b1e2-4c56d6ad2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_path = workdir + 'lalinference_samples.hdf5'\n",
    "out_path = workdir + 'lalinference.out'\n",
    "err_path = workdir + 'lalinference.err'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "656ee3bc-df7b-4147-8312-4b2144e544e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(run_path, \"a\") as run_file:\n",
    "    print('--outfile ' + posterior_path + ' \\\\', file=run_file)\n",
    "    print('--resume \\\\', file=run_file)\n",
    "    print('--progress \\\\', file=run_file)\n",
    "    print('> ' + out_path + ' 2> ' + err_path + ' &', file=run_file, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f257b-3515-4d79-b753-863508e374d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start the inference\n",
    "\n",
    "We will now execute the bash file with a bash command. It is executed from the notebook, but can be started from the terminal as well (in the working directory). The inference process is long and often requires several days to be completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9097cbb-becc-4834-a029-7ba9f5997c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command: bash /home/leila.haegel/open_data/odw_2022/odw-2022/Tutorials/Advanced_topics/tuto_A.1/lalinference_run.sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = 'bash ' + run_path\n",
    "\n",
    "print('Command: ' + command)\n",
    "\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4739abf-59de-4b7e-b0d1-c6399daa7a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lalsuite7.5",
   "language": "python",
   "name": "lalsuite7.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
